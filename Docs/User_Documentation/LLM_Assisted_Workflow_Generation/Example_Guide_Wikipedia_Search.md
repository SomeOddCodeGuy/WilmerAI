## **Example Guide to Prompting Strategies for Writing Wikipedia Searches in Wilmer**

> NOTE: This guide was generated by Gemini 2.5 Pro as a breakdown of how a POC for a recursive Wiki
> workflow works. I'm still testing the workflow, so it has not been released, but I've put this here
> in case anyone wants to look over how it works. So far it's been working great, though.

This guide outlines a modular, multi-pass approach to building a Wikipedia research workflow in WilmerAI. The core
strategy is to break down the complex task of "researching a topic" into a series of smaller, distinct steps, each
handled by a specialized LLM agent or utility node. This creates a resilient and powerful system that iteratively
refines its search, synthesizes information, and provides well-supported answers.

This guide can help not just for writing wiki workflows, but generally understanding Wilmer prompting strategies
when writing other similar types of content.

***

## Phase 1: Contextual Analysis - The Starting Point

Before any research begins, you must first understand the user's true intent. A single user message often lacks context.
The first step in any research workflow should be to create an initial "analysis" node that examines the recent
conversation history.

### **Prompting Strategy: The Analyst Agent**

The goal of this node is to produce a clear, actionable research objective.

1. **Assign a Clear Role**: The system prompt should instruct the LLM to act as an expert analyst (e.g., *"When given an
   ongoing conversation, expertly outline exactly what the user is asking for or saying."*).
2. **Provide Structured Context**: Use XML-style tags like `<recent_conversation>` to clearly delineate the
   conversational data you are providing (e.g., `{chat_user_prompt_last_ten}`).
3. **Give Explicit Instructions**: Guide the LLM's thinking process. Explicitly tell it *not* to take the last message
   at face value but to *"consider the entire context that has been provided and think deeply about what the user might
   really be saying."*
4. **Demand a Clear Output**: Instruct the model to clearly state both its interpretation of the user's request and what
   specific topics should be researched as a result.

The output of this single `Standard` node becomes the master "research objective" that will be passed through the entire
subsequent workflow chain.

***

## Phase 2: The Main Orchestrator Workflow

After establishing the research objective, the request is handed off to a main orchestrator workflow. Let's call it
`Wiki_Recursive_Multi_Pass_Workflow`. This workflow doesn't perform the research itself; it manages the high-level
process, including checking for cached results and deciding whether to initiate a new research loop.

1. **Check the Cache**: Before engaging LLMs, use a `GetCustomFile` node to read a local text file (e.g.,
   `Wiki_Notepad.txt`). This file acts as a simple cache, holding the results of the most recent research
   inquiry.
2. **Make a Decision**: Use a `Standard` node to create a "Decider Agent."
    * **Prompting Strategy**: Provide it with both the current **research objective** (from Phase 1) and the **cached
      notes** (from the file). Instruct it to compare the two and respond with a single, constrained keyword:
      `WIKI_NEEDED` if the notes are insufficient, or `NOTES_GOOD` if the existing information covers the request. This
      constrained output is critical for reliable routing.
3. **Initialize State**: Use `StaticResponse` nodes to initialize variables that will be used if a new research loop is
   required. These include a loop counter (initialized to `0`), an empty string for accumulated results, and an empty
   list for article titles to exclude.
4. **Route the Request**: Use a `ConditionalCustomWorkflow` node that keys off the output of the "Decider Agent."
    * If the output was `NOTES_GOOD`, it calls a simple "responder" workflow that just returns the content of the cached
      notes.
    * If the output was `WIKI_NEEDED`, it calls the main iterative research workflow (see Phase 3), passing along the
      research objective and the initialized state variables.
5. **Save the Results**: After the research workflow completes, use a `SaveCustomFile` node to write the final,
   synthesized result back to the cache file, overwriting the old content. This ensures the cache is always up-to-date
   for the next request.

***

## Phase 3: The Iterative Research Loop

This is the core of the research engine. Create a workflow named something like `Wiki_Iterative_Research`. This workflow
is designed to be called recursively, allowing it to perform multiple "passes" of research, refining its search each
time.

### **Step 1: Generate Search Keywords**

The quality of the research depends entirely on the quality of the search query. This process is split into two paths:
one for the very first search pass, and another for all subsequent, refining passes.

* **Prompting Strategy (First Pass)**: The initial goal is to find a broad, canonical article.
    1. **Role-Play**: "You are a world-class research strategist."
    2. **Explain the Constraints**: Tell the LLM *how* the search tool works. State explicitly that it's a *
       *keyword-based search**, not semantic, and that complex queries fail. This is crucial for getting good, simple
       queries.
    3. **Provide Examples**: Show the LLM what success looks like (e.g., for 'best sci-fi movies ever', the correct
       query is 'Science fiction film').
    4. **Extract the Query**: Use a final LLM node to extract *only the query* from the strategist's longer explanation,
       ensuring a clean string is passed to the search tool.

* **Prompting Strategy (Subsequent Passes)**: The goal now is to find new information and avoid re-reading old articles.
    1. **Provide Full Context**: Give the LLM the original research objective *and* all the results from previous
       passes (accumulated results).
    2. **Define the Goal**: Instruct the LLM to find a **knowledge gap** and generate a **semantically different** query
       that targets this gap.
    3. **Leverage Previous Suggestions**: The summarizer in the next step will suggest new keywords. Tell this keyword
       generator to pay close attention to those suggestions.

### **Step 2: Search, Select, and Summarize**

Once a keyword is generated, you need to find the best article and extract its information. This is best encapsulated in
its own child workflow, like `Util_Workflow_Wiki_Select_And_Summarize`.

1. **Broad Search**: Use the `OfflineWikiApiPartialArticle` node to retrieve the top 10 article summaries for the
   generated keyword. This is more reliable than hoping the single best match is the correct one.
2. **Select the Best Candidate**: Use a `Standard` node as a "Selector Agent."
    * **Prompting Strategy**: Give this agent the research objective, the list of 10 summaries, and—most importantly—a
      list of **article titles that have already been read**. Instruct it to pick the single best article that is **not
      ** on the exclusion list. Force it to respond with either the exact article title or the phrase
      `NO_NOVEL_ARTICLE_FOUND`.
3. **Retrieve and Summarize**: Route the request based on the selector's output. If a title was selected, call another
   child workflow (e.g., `Util_Workflow_Wiki_Retrieve_And_Summarize`).
    * This workflow uses `OfflineWikiApiFullArticle` to get the full text.
    * It then passes this text to a "Summarizer Agent." The summarizer's prompt instructs it to summarize information
      relevant to the research objective and, critically, to **suggest new topics or keywords** found within the article
      for future research passes. This suggestion feeds back into the iterative loop.

### **Step 3: Loop Control and Recursion**

After a research pass is complete, the `Wiki_Iterative_Research` workflow must decide whether to continue.

1. **Decide to Continue**: Use another `Standard` "Decider Agent." Give it the original objective, the accumulated
   results, and the summary from the latest pass. Ask it to respond with only `CONTINUE` or `STOP` based on whether the
   research seems complete or if promising new leads were found.
2. **Update State**: Use `ArithmeticProcessor` to increment the loop counter and `StringConcatenator` to append the
   latest results to the accumulated log.
3. **Check Conditions**: Use a `Conditional` node to check two things: `is max_iterations_reached == FALSE` AND
   `is decider_output == CONTINUE`.
4. **Recurse or Synthesize**: Use a `ConditionalCustomWorkflow`. If the condition is `TRUE`, it calls itself (
   `Wiki_Iterative_Research`), passing in the updated state (new counter, new accumulated results, etc.). If `FALSE`, it
   breaks the loop and calls the final synthesis workflow.

***

## Phase 4: Final Synthesis

Once the loop terminates, a workflow like `Wiki_Synthesis` is called.

### **Prompting Strategy: The Synthesizer Agent**

This agent's only job is to assemble all the gathered information into a coherent, final answer.

1. **Provide All Research**: Give the LLM the complete log of accumulated results from all successful research passes,
   clearly delineated in `<wiki_research_results>` tags.
2. **Set Strict Rules**:
    * Instruct it to base its response *only* on the provided information and not to add or embellish facts.
    * Tell it how to handle failures (e.g., "If the research yielded no relevant information, state that...").
    * Enforce a specific format for citing sources, instructing it to list the titles of the articles it used under a
      specific heading.
3. **Provide the Original Objective**: Finally, provide the original analysis from Phase 1 so the LLM knows what
   question it is supposed to be answering with all the synthesized data.

By following this modular, agent-based structure, you can build a workflow that is far more capable and reliable than a
single, monolithic prompt. It allows the system to think, plan, act, and refine its approach, leading to higher-quality,
fact-based results.